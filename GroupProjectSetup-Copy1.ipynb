{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7654b128",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76f402",
   "metadata": {},
   "source": [
    "                                                    Project details                                                                \n",
    "                                 \n",
    "The data represents various brain activities: resting, math & story tasks, working memory, and motor tasks.\n",
    "\n",
    "    The 'Intra' folder contains data from one subject, while the 'Cross' folder includes multiple subjects.\n",
    "\n",
    "Each file is a matrix of shape 248 x 35624, where 248 represents the number of sensors, and 35624 represents time steps.\n",
    "\n",
    "The files have the following format: “taskType subjectIdentifier number.h5”\n",
    "where taskType can be rest, task motor, task story math, and task working memory.\n",
    "\n",
    "In practice, these tasks correspond to the activities performed by the subjects:\n",
    "\n",
    "    • Resting Task\n",
    "Recording the subjects’ brain while in a relaxed resting\n",
    "state.\n",
    "\n",
    "    • Math & Story Task\n",
    "Subject performs mental calculation and language\n",
    "processing task.\n",
    "\n",
    "    • Working Memory task\n",
    "Subject performs a memorization task.\n",
    "\n",
    "    • Motor Task\n",
    "Subject performs a motor task, typically moving fingers\n",
    "or feets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a095d564",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import Tuple, List, Callable, Optional\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4a67a",
   "metadata": {},
   "source": [
    "Reading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(file_name_with_dir):\n",
    "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f3d43",
   "metadata": {},
   "source": [
    "## Functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cefcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scaling\n",
    "def minmax(trial):\n",
    "    min = trial.min()\n",
    "    max = trial.max()\n",
    "    normalisedTrial = (trial - min)/(max-min)\n",
    "    return normalisedTrial\n",
    "\n",
    "#Z-score normalisation OPTIONAL\n",
    "def zscore(trial):\n",
    "    mean = trial.mean()\n",
    "    sd = trial.std()\n",
    "    normalisedTrial = (trial - mean)/sd \n",
    "    return normalisedTrial\n",
    "\n",
    "#downsamples data by totaltimesteps/factor\n",
    "def downsample(trial, factor):\n",
    "    ds_trial = trial[:,::factor]\n",
    "    return ds_trial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8792f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_files(files = None, path = 'Final Project data/Cross/train', downsampling = 30):\n",
    "    label_to_int = {'rest': 0, 'task_motor': 1, 'task_story_math': 2, 'task_working_memory': 3}\n",
    "\n",
    "    cross_data_train = [] # Store data\n",
    "    cross_data_train_labels = [] # Store labels (based on filename)\n",
    "\n",
    "    if files == None:\n",
    "        files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = f'{path}/{file}'\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as h5_file:\n",
    "            # obtain labels\n",
    "            dataset_name = get_dataset_name(file_path)\n",
    "            label = dataset_name.split('_')\n",
    "            label.remove(label[len(label)-1])\n",
    "            label = '_'.join(label)\n",
    "            cross_data_train_labels.append(label_to_int[label])\n",
    "            \n",
    "            # obtain X_data\n",
    "            matrix = h5_file.get(dataset_name)[()]\n",
    "            normalisedMatrix = downsample(zscore(matrix), downsampling) # apply minmax normalisation and downsampling\n",
    "            cross_data_train.append(normalisedMatrix.T) # Transpose\n",
    "             \n",
    "    X = torch.from_numpy(np.array(cross_data_train)).float()\n",
    "    y = torch.tensor(cross_data_train_labels)        \n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb9dc3",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520ca917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through RNN\n",
    "        rnn, _ = self.rnn(x)\n",
    "        \n",
    "        # Only take the output from the final time step\n",
    "        output = self.fc(rnn[:, -1, :])\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b27d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams: lr, hidden_size, downsampling\n",
    "def train(path, lr = 0.001, hidden_size = 200, downsampling = 30):\n",
    "    random.seed = 123 # Set seed for reproducability\n",
    "    input_size = 248\n",
    "    output_size = 4\n",
    "    network = RNN(input_size, hidden_size, output_size)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    random.shuffle(files) # Shuffle order of files\n",
    "    current_samples = []\n",
    "    n = 8\n",
    "    batch_index = 1\n",
    "\n",
    "    for i, file in tqdm(enumerate(files)):\n",
    "        current_samples.append(file)\n",
    "        if len(current_samples) == n or i == (len(files)-1):\n",
    "            print(f\"training batch {batch_index}...\")\n",
    "            X_train, y_train = preprocess_files(current_samples, downsampling=downsampling) \n",
    "            current_samples = []\n",
    "            \n",
    "            network.train()\n",
    "            opt.zero_grad()\n",
    "            output = network(X_train)\n",
    "            loss = loss_fn(output, y_train)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            batch_index += 1\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e3fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing:\n",
    "def test(network, paths):\n",
    "    for path in paths:\n",
    "        files = os.listdir(path)\n",
    "        X, y = preprocess_files(files, path, 1)\n",
    "        network.eval()\n",
    "\n",
    "        test_output = network(X).detach().numpy()\n",
    "        pred = np.argmax(test_output, axis=1) # to numpy\n",
    "        y = y.numpy()\n",
    "        return accuracy_score(pred, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286ae58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:03,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:07,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:10,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:16,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:20,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [00:23,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:25,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "path = 'Final Project data/Cross/train'\n",
    "network = train(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4ac940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [ 'Final Project data/Cross/test1',  'Final Project data/Cross/test2',  'Final Project data/Cross/test3']\n",
    "test(network=network, paths=paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7a9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7654b128",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76f402",
   "metadata": {},
   "source": [
    "                                                    Project details                                                                \n",
    "                                 \n",
    "The data represents various brain activities: resting, math & story tasks, working memory, and motor tasks.\n",
    "\n",
    "    The 'Intra' folder contains data from one subject, while the 'Cross' folder includes multiple subjects.\n",
    "\n",
    "Each file is a matrix of shape 248 x 35624, where 248 represents the number of sensors, and 35624 represents time steps.\n",
    "\n",
    "The files have the following format: “taskType subjectIdentifier number.h5”\n",
    "where taskType can be rest, task motor, task story math, and task working memory.\n",
    "\n",
    "In practice, these tasks correspond to the activities performed by the subjects:\n",
    "\n",
    "    • Resting Task\n",
    "Recording the subjects’ brain while in a relaxed resting\n",
    "state.\n",
    "\n",
    "    • Math & Story Task\n",
    "Subject performs mental calculation and language\n",
    "processing task.\n",
    "\n",
    "    • Working Memory task\n",
    "Subject performs a memorization task.\n",
    "\n",
    "    • Motor Task\n",
    "Subject performs a motor task, typically moving fingers\n",
    "or feets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a095d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import Tuple, List, Callable, Optional\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4a67a",
   "metadata": {},
   "source": [
    "Reading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "447432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(file_name_with_dir):\n",
    "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9cefcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# min-max scaling\n",
    "def minmax(trial):\n",
    "    min = trial.min()\n",
    "    max = trial.max()\n",
    "    normalisedTrial = (trial - min)/(max-min)\n",
    "    return normalisedTrial\n",
    "\n",
    "#Z-score normalisation OPTIONAL\n",
    "def zscore(trial):\n",
    "    mean = trial.mean()\n",
    "    sd = trial.std()\n",
    "    normalisedTrial = (trial - mean)/sd \n",
    "    return normalisedTrial\n",
    "\n",
    "#downsamples data by totaltimesteps/factor\n",
    "def downsample(trial, factor):\n",
    "    ds_trial = trial[:,::factor]\n",
    "    return ds_trial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "520ca917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through RNN\n",
    "        rnn, _ = self.rnn(x)\n",
    "        \n",
    "        # Only take the output from the final time step\n",
    "        output = self.fc(rnn[:, -1, :])\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "75a55a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for storing data in a folder into an array\n",
    "\n",
    "\n",
    "def preprocess_files(files = None, path = 'Final Project data/Cross/train', downsampling = 30):\n",
    "    label_to_int = {'rest': 0, 'task_motor': 1, 'task_story_math': 2, 'task_working_memory': 3}\n",
    "\n",
    "    cross_data_train = [] # Store data\n",
    "    cross_data_train_labels = [] # Store labels (based on filename)\n",
    "\n",
    "    if files == None:\n",
    "        files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = f'{path}/{file}'\n",
    "        \n",
    "        with h5py.File(file_path, 'r') as h5_file:\n",
    "            # obtain labels\n",
    "            dataset_name = get_dataset_name(file_path)\n",
    "            label = dataset_name.split('_')\n",
    "            label.remove(label[len(label)-1])\n",
    "            label = '_'.join(label)\n",
    "            cross_data_train_labels.append(label_to_int[label])\n",
    "            \n",
    "            # obtain X_data\n",
    "            matrix = h5_file.get(dataset_name)[()]\n",
    "            normalisedMatrix = downsample(zscore(matrix), downsampling) # apply minmax normalisation and downsampling\n",
    "            cross_data_train.append(normalisedMatrix.T) # Transpose\n",
    "             \n",
    "    X = torch.from_numpy(np.array(cross_data_train)).float()\n",
    "    y = torch.tensor(cross_data_train_labels)        \n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94b27d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:32,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:05,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:38,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [02:14,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [02:52,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [03:27,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [04:02,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [04:36,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed = 123 # Set seed for reproducability\n",
    "input_size = 248\n",
    "hidden_size = 200\n",
    "output_size = 4\n",
    "network = RNN(input_size, hidden_size, output_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "path = 'Final Project data/Cross/train'\n",
    "files = os.listdir(path)\n",
    "random.shuffle(files) # Shuffle order of files\n",
    "current_samples = []\n",
    "n = 8\n",
    "batch_index = 1\n",
    "\n",
    "for i, file in tqdm(enumerate(files)):\n",
    "    current_samples.append(file)\n",
    "    if len(current_samples) == n or i == (len(files)-1):\n",
    "        print(f\"training batch {batch_index}...\")\n",
    "        X_train, y_train = preprocess_files(current_samples, downsampling=1) \n",
    "        current_samples = []\n",
    "        \n",
    "        network.train()\n",
    "        opt.zero_grad()\n",
    "        output = network(X_train)\n",
    "        loss = loss_fn(output, y_train)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        batch_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "10e3fe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rest_162935_1.h5', 'rest_162935_10.h5', 'rest_162935_3.h5', 'rest_162935_5.h5', 'task_motor_162935_1.h5', 'task_motor_162935_3.h5', 'task_motor_162935_4.h5', 'task_motor_162935_9.h5', 'task_story_math_162935_2.h5', 'task_story_math_162935_3.h5', 'task_story_math_162935_4.h5', 'task_story_math_162935_6.h5', 'task_working_memory_162935_3.h5', 'task_working_memory_162935_4.h5', 'task_working_memory_162935_5.h5', 'task_working_memory_162935_7.h5']\n",
      "[0 0 0 0 3 3 3 3 2 1 2 2 3 3 3 3]\n",
      "0.6875\n",
      "['rest_707749_10.h5', 'rest_707749_4.h5', 'rest_707749_5.h5', 'rest_707749_7.h5', 'task_motor_707749_2.h5', 'task_motor_707749_7.h5', 'task_motor_707749_8.h5', 'task_motor_707749_9.h5', 'task_story_math_707749_10.h5', 'task_story_math_707749_2.h5', 'task_story_math_707749_5.h5', 'task_story_math_707749_6.h5', 'task_working_memory_707749_10.h5', 'task_working_memory_707749_4.h5', 'task_working_memory_707749_8.h5', 'task_working_memory_707749_9.h5']\n",
      "[0 0 0 0 3 3 3 3 3 3 3 3 3 0 3 3]\n",
      "0.4375\n",
      "['rest_725751_10.h5', 'rest_725751_4.h5', 'rest_735148_4.h5', 'rest_735148_9.h5', 'task_motor_725751_1.h5', 'task_motor_725751_9.h5', 'task_motor_735148_2.h5', 'task_motor_735148_7.h5', 'task_story_math_725751_1.h5', 'task_story_math_725751_7.h5', 'task_story_math_735148_3.h5', 'task_story_math_735148_5.h5', 'task_working_memory_725751_3.h5', 'task_working_memory_725751_8.h5', 'task_working_memory_735148_2.h5', 'task_working_memory_735148_4.h5']\n",
      "[0 0 0 0 3 3 3 3 1 3 3 3 3 3 3 3]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# testing:\n",
    "paths = [ 'Final Project data/Cross/test1',  'Final Project data/Cross/test2',  'Final Project data/Cross/test3']\n",
    "for path in paths:\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    X, y = preprocess_files(files, path, 1)\n",
    "    network.eval()\n",
    "\n",
    "    test_output = network(X).detach().numpy()\n",
    "    pred = np.argmax(test_output, axis=1) # to numpy\n",
    "    y = y.numpy()\n",
    "    print(pred)\n",
    "    print(accuracy_score(pred, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ac940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7a9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
